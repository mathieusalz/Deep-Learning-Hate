TODO last week :
- train the model on all language except 1 and evaluate on the language that he has never seen//Timothée
- poster//tout le monde

Limitations :
- train avec un plus petit set de données pour faire la comparaison et montrer que plus de donnée -> meilleurs résultats //Timothée
- mauvaise annotation des datasets
- un seul BERT -> maybe avoir un tokenizer spécialisé dans les langues sur lesquelles on train
- each dataset is imbalanced in its own way --> calculating class imbalance on global dataset wrong way to go
